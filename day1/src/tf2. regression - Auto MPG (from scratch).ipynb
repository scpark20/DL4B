{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Abstract Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def backward(self, accum_grad, learning_rate):\n",
    "        raise NotImplementedError()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Reference : https://keras.io/api/layers/initializers/\n",
    "        #limit = np.sqrt(6/(input_dim+output_dim))\n",
    "        #self.W = np.random.uniform(-limit, limit, size=(input_dim, output_dim))\n",
    "        self.W = np.random.randn(input_dim, output_dim) * 0.1\n",
    "        self.b = np.zeros(output_dim)\n",
    "        self.x_save = None # saving input x in forward step for using in backward step\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x : (batch, input_dim)\n",
    "        self.x_save = x\n",
    "        \n",
    "        # Dense layer를 수식으로 표현하면 다음과 같습니다.\n",
    "        # y = x@W + b\n",
    "        \n",
    "        # (batch, output_dim)\n",
    "        y = x @ self.W + self.b\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    def backward(self, accum_grad, learning_rate):\n",
    "        # accum_grad : ∂L/∂y, (batch, output_dim)\n",
    "        \n",
    "        # y를 W에 대해 미분하면 다음과 같이 되고,\n",
    "        # ∂y/∂W = x\n",
    "        \n",
    "        # Loss를 W에 대해 미분하면 chain rule에 의해 다음과 같이 표현됩니다.\n",
    "        # ∂L/∂W = ∂L/∂y·∂y/∂W = ∂L/∂y·x\n",
    "        \n",
    "        # (input_dim, output_dim)\n",
    "        W_grad = self.x_save.T @ accum_grad\n",
    "        \n",
    "        # 또한 y를 b에 대해 미분하면 다음과 같이 되고,\n",
    "        # ∂y/∂b = 1\n",
    "        \n",
    "        # Loss를 b에 대해 미분하면 chain rule에 의해 다음과 같이 표현됩니다.\n",
    "        # ∂L/∂b = ∂L/∂y·∂y/∂b = ∂L/∂y·1\n",
    "        b_grad = np.sum(accum_grad, axis=0)\n",
    "        \n",
    "        # Loss를 x에 대해 미분하면 chain rule에 의해 다음과 같이 표현됩니다.\n",
    "        # ∂L/∂x = ∂L/∂y·∂y/∂x\n",
    "        # y를 x에 대해 미분하면 다음과 같이 됩니다.\n",
    "        # ∂y/∂x = W\n",
    "        accum_grad = accum_grad @ self.W.T\n",
    "\n",
    "        # Weight&bias 업데이트\n",
    "        self.W = self.W - learning_rate * W_grad\n",
    "        self.b = self.b - learning_rate * b_grad\n",
    "        \n",
    "        return accum_grad\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/deep_learning/activation_functions.py\n",
    "class Sigmoid(Layer):\n",
    "    def forward(self, x):\n",
    "        self.x_save = x\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, accum_grad, learning_rate):\n",
    "        grad = self.forward(self.x_save) * (1 - self.forward(self.x_save))\n",
    "        return accum_grad * grad\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def forward(self, x):\n",
    "        self.x_save = x\n",
    "        return np.where(x>0, x, 0)\n",
    "    \n",
    "    def backward(self, accum_grad, learning_rate):\n",
    "        grad = np.where(self.x_save>0, 1, 0)\n",
    "        return accum_grad * grad\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    def forward(self, x):\n",
    "        self.x_save = x\n",
    "        return 2 / (1 + np.exp(-2*x)) - 1\n",
    "    \n",
    "    def backward(self, accum_grad, learning_rate):\n",
    "        grad = 1 - np.power(self.forward(self.x_save), 2)\n",
    "        return accum_grad * grad\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Layer):\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x : (batch, input_dim)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def backward(self, accum_grad, learning_rate):\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            accum_grad = layer.backward(accum_grad, learning_rate)\n",
    "            \n",
    "        return accum_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquareError():\n",
    "    @staticmethod\n",
    "    def get_loss(y_pred, y_true):\n",
    "        # y_pred : (batch, dim)\n",
    "        # y_true : (batch, dim)\n",
    "        \n",
    "        # scalar\n",
    "        loss = 0.5 * np.mean((y_pred - y_true) ** 2)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gradient(y_pred, y_true):\n",
    "        grad = -y_true + y_pred\n",
    "        \n",
    "        return grad\n",
    "    \n",
    "from scipy.special import softmax\n",
    "class SparseCrossEntropy():\n",
    "    # Reference : https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1\n",
    "    @staticmethod\n",
    "    def get_loss(y_pred, y_true):\n",
    "        # y_pred : (batch, dim)\n",
    "        # y_true : (batch,)\n",
    "        \n",
    "        eps = np.finfo(float).eps\n",
    "        log_p = np.log(softmax(y_pred, axis=1) + eps)\n",
    "        \n",
    "        negative_log_likelihoods = []\n",
    "        for i, target in enumerate(y_true):\n",
    "            nll = -log_p[i, target]\n",
    "            negative_log_likelihoods.append(nll)\n",
    "            \n",
    "        loss = np.mean(negative_log_likelihoods)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gradient(y_pred, y_true):\n",
    "        # y_pred : (batch, dim)\n",
    "        # y_true : (batch,)\n",
    "        \n",
    "        y_true = np.eye(y_pred.shape[1])[y_true]\n",
    "        grad = softmax(y_pred, axis=1) - y_true\n",
    "        return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318, 9) (80, 9)\n",
      "(318,) (80,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
    "\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight', 'Acceleration', 'Model Year', 'Origin']\n",
    "raw_dataset = pd.read_csv(dataset_path, names=column_names, na_values = \"?\", comment='\\t', sep=\" \", skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset = dataset.fillna(dataset.mean())\n",
    "\n",
    "origin = dataset.pop('Origin')\n",
    "dataset['USA'] = (origin == 1)*1.0\n",
    "dataset['Europe'] = (origin == 2)*1.0\n",
    "dataset['Japan'] = (origin == 3)*1.0\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "train_labels = train_dataset.pop('MPG')\n",
    "test_labels = test_dataset.pop('MPG')\n",
    "\n",
    "train_stats = train_dataset.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "\n",
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data = norm(test_dataset)\n",
    "\n",
    "print(normed_train_data.shape, normed_test_data.shape)\n",
    "print(train_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Sequential object at 0x7f1ffee94760>\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([Dense(9, 64),\n",
    "                    ReLU(),\n",
    "                    Dense(64, 64),\n",
    "                    ReLU(),\n",
    "                    Dense(64, 1),\n",
    "                   ])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (Fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Loss : 308.8750141590762\n",
      "Epoch : 1 Loss : 305.2696688877288\n",
      "Epoch : 2 Loss : 301.5952089712653\n",
      "Epoch : 3 Loss : 297.71695014336785\n",
      "Epoch : 4 Loss : 293.4801140667641\n",
      "Epoch : 5 Loss : 288.6907577832131\n",
      "Epoch : 6 Loss : 283.11919959030075\n",
      "Epoch : 7 Loss : 276.4470631669098\n",
      "Epoch : 8 Loss : 268.27311328519244\n",
      "Epoch : 9 Loss : 257.996535400068\n",
      "Epoch : 10 Loss : 244.82535439609157\n",
      "Epoch : 11 Loss : 227.69268420582407\n",
      "Epoch : 12 Loss : 205.395792202185\n",
      "Epoch : 13 Loss : 176.8395486445952\n",
      "Epoch : 14 Loss : 141.97293024126262\n",
      "Epoch : 15 Loss : 103.84339817365816\n",
      "Epoch : 16 Loss : 69.40289540758397\n",
      "Epoch : 17 Loss : 46.106522209646634\n",
      "Epoch : 18 Loss : 34.297721736612864\n",
      "Epoch : 19 Loss : 28.150657293177872\n",
      "Epoch : 20 Loss : 23.714815827883395\n",
      "Epoch : 21 Loss : 20.039279142471482\n",
      "Epoch : 22 Loss : 16.99637053303391\n",
      "Epoch : 23 Loss : 14.552053572780077\n",
      "Epoch : 24 Loss : 12.639820539908495\n",
      "Epoch : 25 Loss : 11.175381621864597\n",
      "Epoch : 26 Loss : 10.07597132521288\n",
      "Epoch : 27 Loss : 9.257308157389355\n",
      "Epoch : 28 Loss : 8.646344982769367\n",
      "Epoch : 29 Loss : 8.182825422217991\n",
      "Epoch : 30 Loss : 7.8192040895472505\n",
      "Epoch : 31 Loss : 7.524977843193429\n",
      "Epoch : 32 Loss : 7.278779000278546\n",
      "Epoch : 33 Loss : 7.0669624577436725\n",
      "Epoch : 34 Loss : 6.879912115480555\n",
      "Epoch : 35 Loss : 6.711871724927273\n",
      "Epoch : 36 Loss : 6.5588716078761875\n",
      "Epoch : 37 Loss : 6.417522065975931\n",
      "Epoch : 38 Loss : 6.286330378894687\n",
      "Epoch : 39 Loss : 6.16346942349049\n",
      "Epoch : 40 Loss : 6.04875549469278\n",
      "Epoch : 41 Loss : 5.941320375678968\n",
      "Epoch : 42 Loss : 5.8404169295374055\n",
      "Epoch : 43 Loss : 5.745403473118299\n",
      "Epoch : 44 Loss : 5.655841659648498\n",
      "Epoch : 45 Loss : 5.571399417141525\n",
      "Epoch : 46 Loss : 5.491464339490619\n",
      "Epoch : 47 Loss : 5.415244276108293\n",
      "Epoch : 48 Loss : 5.342970554444541\n",
      "Epoch : 49 Loss : 5.274688498106297\n",
      "Epoch : 50 Loss : 5.209856907434422\n",
      "Epoch : 51 Loss : 5.148471503332642\n",
      "Epoch : 52 Loss : 5.090871018012482\n",
      "Epoch : 53 Loss : 5.035850717306548\n",
      "Epoch : 54 Loss : 4.98341819967906\n",
      "Epoch : 55 Loss : 4.933342840063735\n",
      "Epoch : 56 Loss : 4.8855426734217495\n",
      "Epoch : 57 Loss : 4.83988918854842\n",
      "Epoch : 58 Loss : 4.796204511178346\n",
      "Epoch : 59 Loss : 4.754453311446334\n",
      "Epoch : 60 Loss : 4.714897924000527\n",
      "Epoch : 61 Loss : 4.677326688678071\n",
      "Epoch : 62 Loss : 4.641545986446107\n",
      "Epoch : 63 Loss : 4.60760199276709\n",
      "Epoch : 64 Loss : 4.57486588017049\n",
      "Epoch : 65 Loss : 4.543546452916265\n",
      "Epoch : 66 Loss : 4.513433467593676\n",
      "Epoch : 67 Loss : 4.484527008589986\n",
      "Epoch : 68 Loss : 4.4569630276696435\n",
      "Epoch : 69 Loss : 4.430560327228704\n",
      "Epoch : 70 Loss : 4.40535616111318\n",
      "Epoch : 71 Loss : 4.3813869683892115\n",
      "Epoch : 72 Loss : 4.358305691221384\n",
      "Epoch : 73 Loss : 4.335937148979138\n",
      "Epoch : 74 Loss : 4.314314200820885\n",
      "Epoch : 75 Loss : 4.293282060165752\n",
      "Epoch : 76 Loss : 4.2730203934431215\n",
      "Epoch : 77 Loss : 4.253478353911959\n",
      "Epoch : 78 Loss : 4.2345850336275035\n",
      "Epoch : 79 Loss : 4.216434077352391\n",
      "Epoch : 80 Loss : 4.199029248222657\n",
      "Epoch : 81 Loss : 4.18228134382653\n",
      "Epoch : 82 Loss : 4.166061781741718\n",
      "Epoch : 83 Loss : 4.150206975406458\n",
      "Epoch : 84 Loss : 4.13482256278811\n",
      "Epoch : 85 Loss : 4.120089424110154\n",
      "Epoch : 86 Loss : 4.105842787514644\n",
      "Epoch : 87 Loss : 4.091676920589264\n",
      "Epoch : 88 Loss : 4.07785972554573\n",
      "Epoch : 89 Loss : 4.064508905242313\n",
      "Epoch : 90 Loss : 4.051567815289397\n",
      "Epoch : 91 Loss : 4.038934683920536\n",
      "Epoch : 92 Loss : 4.026578762146064\n",
      "Epoch : 93 Loss : 4.014562477726198\n",
      "Epoch : 94 Loss : 4.002901589374116\n",
      "Epoch : 95 Loss : 3.991778386665943\n",
      "Epoch : 96 Loss : 3.981041843896337\n",
      "Epoch : 97 Loss : 3.9706478761505437\n",
      "Epoch : 98 Loss : 3.96036586853536\n",
      "Epoch : 99 Loss : 3.950320764328864\n",
      "Epoch : 100 Loss : 3.9405775960461193\n",
      "Epoch : 101 Loss : 3.9311298258054785\n",
      "Epoch : 102 Loss : 3.9219566671178323\n",
      "Epoch : 103 Loss : 3.9131153925961755\n",
      "Epoch : 104 Loss : 3.9044873522621346\n",
      "Epoch : 105 Loss : 3.8960591576447703\n",
      "Epoch : 106 Loss : 3.8878769995663007\n",
      "Epoch : 107 Loss : 3.879840819774881\n",
      "Epoch : 108 Loss : 3.8720820598052654\n",
      "Epoch : 109 Loss : 3.864512717878145\n",
      "Epoch : 110 Loss : 3.8571188347406555\n",
      "Epoch : 111 Loss : 3.849888833916941\n",
      "Epoch : 112 Loss : 3.842808200755174\n",
      "Epoch : 113 Loss : 3.8359315787196597\n",
      "Epoch : 114 Loss : 3.8292193617968606\n",
      "Epoch : 115 Loss : 3.822542191454839\n",
      "Epoch : 116 Loss : 3.815767239758483\n",
      "Epoch : 117 Loss : 3.8091557738975412\n",
      "Epoch : 118 Loss : 3.8026984139078515\n",
      "Epoch : 119 Loss : 3.796469878035224\n",
      "Epoch : 120 Loss : 3.7903596927482464\n",
      "Epoch : 121 Loss : 3.784369870632776\n",
      "Epoch : 122 Loss : 3.778478746245388\n",
      "Epoch : 123 Loss : 3.7726714724916786\n",
      "Epoch : 124 Loss : 3.767062038302537\n",
      "Epoch : 125 Loss : 3.7615801262094117\n",
      "Epoch : 126 Loss : 3.756209847248034\n",
      "Epoch : 127 Loss : 3.750911818842771\n",
      "Epoch : 128 Loss : 3.7456671465751836\n",
      "Epoch : 129 Loss : 3.740500614418325\n",
      "Epoch : 130 Loss : 3.7354148115890657\n",
      "Epoch : 131 Loss : 3.730438012788181\n",
      "Epoch : 132 Loss : 3.725467244715688\n",
      "Epoch : 133 Loss : 3.720567661992019\n",
      "Epoch : 134 Loss : 3.715843815388432\n",
      "Epoch : 135 Loss : 3.7112800614977965\n",
      "Epoch : 136 Loss : 3.70674453828808\n",
      "Epoch : 137 Loss : 3.7022198837732527\n",
      "Epoch : 138 Loss : 3.6977938889233086\n",
      "Epoch : 139 Loss : 3.6934302597225934\n",
      "Epoch : 140 Loss : 3.689138780909534\n",
      "Epoch : 141 Loss : 3.6849104437267357\n",
      "Epoch : 142 Loss : 3.680738212919543\n",
      "Epoch : 143 Loss : 3.67663237650256\n",
      "Epoch : 144 Loss : 3.6725900249712753\n",
      "Epoch : 145 Loss : 3.6686021320569933\n",
      "Epoch : 146 Loss : 3.6646834679580698\n",
      "Epoch : 147 Loss : 3.660856942802464\n",
      "Epoch : 148 Loss : 3.6571657654952867\n",
      "Epoch : 149 Loss : 3.6535543259322383\n",
      "Epoch : 150 Loss : 3.650043312003233\n",
      "Epoch : 151 Loss : 3.646592097974465\n",
      "Epoch : 152 Loss : 3.643103007147108\n",
      "Epoch : 153 Loss : 3.6396732789573703\n",
      "Epoch : 154 Loss : 3.6362373545671187\n",
      "Epoch : 155 Loss : 3.6328037322908804\n",
      "Epoch : 156 Loss : 3.6294538603986077\n",
      "Epoch : 157 Loss : 3.6261202136115407\n",
      "Epoch : 158 Loss : 3.6228625937713006\n",
      "Epoch : 159 Loss : 3.6196145333315917\n",
      "Epoch : 160 Loss : 3.616415331381658\n",
      "Epoch : 161 Loss : 3.6132499196272483\n",
      "Epoch : 162 Loss : 3.610112795575061\n",
      "Epoch : 163 Loss : 3.6070092650893573\n",
      "Epoch : 164 Loss : 3.603952618643057\n",
      "Epoch : 165 Loss : 3.6009633513657056\n",
      "Epoch : 166 Loss : 3.598047490435717\n",
      "Epoch : 167 Loss : 3.595110175487747\n",
      "Epoch : 168 Loss : 3.5921972037747123\n",
      "Epoch : 169 Loss : 3.5893268598593986\n",
      "Epoch : 170 Loss : 3.586458063266285\n",
      "Epoch : 171 Loss : 3.5835956370973627\n",
      "Epoch : 172 Loss : 3.5807715193355283\n",
      "Epoch : 173 Loss : 3.5779824009189083\n",
      "Epoch : 174 Loss : 3.5752030587800627\n",
      "Epoch : 175 Loss : 3.572455632732137\n",
      "Epoch : 176 Loss : 3.5697562151895266\n",
      "Epoch : 177 Loss : 3.5670739535062634\n",
      "Epoch : 178 Loss : 3.5644085139873476\n",
      "Epoch : 179 Loss : 3.561778970943148\n",
      "Epoch : 180 Loss : 3.559153431147448\n",
      "Epoch : 181 Loss : 3.556577148305768\n",
      "Epoch : 182 Loss : 3.5540690674516187\n",
      "Epoch : 183 Loss : 3.5515859397162854\n",
      "Epoch : 184 Loss : 3.5490892446899114\n",
      "Epoch : 185 Loss : 3.54664332823121\n",
      "Epoch : 186 Loss : 3.5442609071463465\n",
      "Epoch : 187 Loss : 3.5418990798605696\n",
      "Epoch : 188 Loss : 3.5395641397955453\n",
      "Epoch : 189 Loss : 3.537259102358444\n",
      "Epoch : 190 Loss : 3.5349286831145195\n",
      "Epoch : 191 Loss : 3.5325885149160206\n",
      "Epoch : 192 Loss : 3.530196461204008\n",
      "Epoch : 193 Loss : 3.527882506220264\n",
      "Epoch : 194 Loss : 3.5256413281949963\n",
      "Epoch : 195 Loss : 3.523386865354069\n",
      "Epoch : 196 Loss : 3.5211547722483716\n",
      "Epoch : 197 Loss : 3.5189653013925\n",
      "Epoch : 198 Loss : 3.516850673518762\n",
      "Epoch : 199 Loss : 3.5147487765868104\n",
      "Epoch : 200 Loss : 3.512664885969743\n",
      "Epoch : 201 Loss : 3.5106058673560767\n",
      "Epoch : 202 Loss : 3.5085006400129175\n",
      "Epoch : 203 Loss : 3.5064236880889297\n",
      "Epoch : 204 Loss : 3.504316405684162\n",
      "Epoch : 205 Loss : 3.502116686661299\n",
      "Epoch : 206 Loss : 3.499917130013164\n",
      "Epoch : 207 Loss : 3.497758785038843\n",
      "Epoch : 208 Loss : 3.4956099415486626\n",
      "Epoch : 209 Loss : 3.4935063262411705\n",
      "Epoch : 210 Loss : 3.491425978917266\n",
      "Epoch : 211 Loss : 3.4893457266584433\n",
      "Epoch : 212 Loss : 3.4872683998222875\n",
      "Epoch : 213 Loss : 3.4851561307297647\n",
      "Epoch : 214 Loss : 3.4830772509695826\n",
      "Epoch : 215 Loss : 3.480982722011363\n",
      "Epoch : 216 Loss : 3.478909738871394\n",
      "Epoch : 217 Loss : 3.476854206852319\n",
      "Epoch : 218 Loss : 3.474813073256524\n",
      "Epoch : 219 Loss : 3.4727657871760447\n",
      "Epoch : 220 Loss : 3.470753806355407\n",
      "Epoch : 221 Loss : 3.4687473016700427\n",
      "Epoch : 222 Loss : 3.466760380113184\n",
      "Epoch : 223 Loss : 3.4647733856130363\n",
      "Epoch : 224 Loss : 3.4628408169343223\n",
      "Epoch : 225 Loss : 3.4609069909465204\n",
      "Epoch : 226 Loss : 3.458999414574668\n",
      "Epoch : 227 Loss : 3.4571056729100724\n",
      "Epoch : 228 Loss : 3.4552299975630145\n",
      "Epoch : 229 Loss : 3.453356245243374\n",
      "Epoch : 230 Loss : 3.4514825697248512\n",
      "Epoch : 231 Loss : 3.4496548340758686\n",
      "Epoch : 232 Loss : 3.4478384505315796\n",
      "Epoch : 233 Loss : 3.4460507251530785\n",
      "Epoch : 234 Loss : 3.4442865722153257\n",
      "Epoch : 235 Loss : 3.4425278393329894\n",
      "Epoch : 236 Loss : 3.440781274664144\n",
      "Epoch : 237 Loss : 3.439053792784676\n",
      "Epoch : 238 Loss : 3.437332165170673\n",
      "Epoch : 239 Loss : 3.4356623684557976\n",
      "Epoch : 240 Loss : 3.4339878949244635\n",
      "Epoch : 241 Loss : 3.4322934052247405\n",
      "Epoch : 242 Loss : 3.4306052797183364\n",
      "Epoch : 243 Loss : 3.4288734455435717\n",
      "Epoch : 244 Loss : 3.427154447528536\n",
      "Epoch : 245 Loss : 3.425428235029434\n",
      "Epoch : 246 Loss : 3.42368078434897\n",
      "Epoch : 247 Loss : 3.421938403641803\n",
      "Epoch : 248 Loss : 3.420203604715322\n",
      "Epoch : 249 Loss : 3.4184913993629906\n",
      "Epoch : 250 Loss : 3.416787642027309\n",
      "Epoch : 251 Loss : 3.415073966578575\n",
      "Epoch : 252 Loss : 3.413397737654101\n",
      "Epoch : 253 Loss : 3.4116777015634945\n",
      "Epoch : 254 Loss : 3.409979421894853\n",
      "Epoch : 255 Loss : 3.4082644964039805\n",
      "Epoch : 256 Loss : 3.4065648402141053\n",
      "Epoch : 257 Loss : 3.404869823588428\n",
      "Epoch : 258 Loss : 3.403180013512329\n",
      "Epoch : 259 Loss : 3.401490069438282\n",
      "Epoch : 260 Loss : 3.3997971241280935\n",
      "Epoch : 261 Loss : 3.3980842976127636\n",
      "Epoch : 262 Loss : 3.396376716448174\n",
      "Epoch : 263 Loss : 3.3946374580191705\n",
      "Epoch : 264 Loss : 3.3929136642658837\n",
      "Epoch : 265 Loss : 3.3912195176000375\n",
      "Epoch : 266 Loss : 3.389515110049156\n",
      "Epoch : 267 Loss : 3.3878300567410595\n",
      "Epoch : 268 Loss : 3.3861538285547734\n",
      "Epoch : 269 Loss : 3.3844453287994605\n",
      "Epoch : 270 Loss : 3.3827188651682545\n",
      "Epoch : 271 Loss : 3.381013316721672\n",
      "Epoch : 272 Loss : 3.379222634930893\n",
      "Epoch : 273 Loss : 3.377468109517288\n",
      "Epoch : 274 Loss : 3.375820703088205\n",
      "Epoch : 275 Loss : 3.3741884672276012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 276 Loss : 3.372565785962474\n",
      "Epoch : 277 Loss : 3.3710125332213843\n",
      "Epoch : 278 Loss : 3.369490977069206\n",
      "Epoch : 279 Loss : 3.367970926848545\n",
      "Epoch : 280 Loss : 3.3664414319528966\n",
      "Epoch : 281 Loss : 3.3649322130512367\n",
      "Epoch : 282 Loss : 3.3634202900318857\n",
      "Epoch : 283 Loss : 3.3618945516832612\n",
      "Epoch : 284 Loss : 3.3603971096642953\n",
      "Epoch : 285 Loss : 3.3589073327549728\n",
      "Epoch : 286 Loss : 3.3574218712025323\n",
      "Epoch : 287 Loss : 3.355951997908993\n",
      "Epoch : 288 Loss : 3.354457751920091\n",
      "Epoch : 289 Loss : 3.3529872806868366\n",
      "Epoch : 290 Loss : 3.351520995622391\n",
      "Epoch : 291 Loss : 3.350043405505493\n",
      "Epoch : 292 Loss : 3.348596695170154\n",
      "Epoch : 293 Loss : 3.3471482184088472\n",
      "Epoch : 294 Loss : 3.345687099316552\n",
      "Epoch : 295 Loss : 3.3442413450987534\n",
      "Epoch : 296 Loss : 3.342771351997795\n",
      "Epoch : 297 Loss : 3.3412983606212654\n",
      "Epoch : 298 Loss : 3.3398070721878144\n",
      "Epoch : 299 Loss : 3.338305623422062\n",
      "Epoch : 300 Loss : 3.3367983464758133\n",
      "Epoch : 301 Loss : 3.335298856843769\n",
      "Epoch : 302 Loss : 3.3337986396821697\n",
      "Epoch : 303 Loss : 3.3323102902056663\n",
      "Epoch : 304 Loss : 3.330815962336704\n",
      "Epoch : 305 Loss : 3.3293289750137727\n",
      "Epoch : 306 Loss : 3.3279009830296675\n",
      "Epoch : 307 Loss : 3.3264503355827215\n",
      "Epoch : 308 Loss : 3.3250352597130854\n",
      "Epoch : 309 Loss : 3.3235862145752706\n",
      "Epoch : 310 Loss : 3.322124906071343\n",
      "Epoch : 311 Loss : 3.3207016115099575\n",
      "Epoch : 312 Loss : 3.319265078123152\n",
      "Epoch : 313 Loss : 3.317846341013669\n",
      "Epoch : 314 Loss : 3.3164645885955784\n",
      "Epoch : 315 Loss : 3.3150919893569766\n",
      "Epoch : 316 Loss : 3.313659649982447\n",
      "Epoch : 317 Loss : 3.3122618224619855\n",
      "Epoch : 318 Loss : 3.3108570985977233\n",
      "Epoch : 319 Loss : 3.309457039155394\n",
      "Epoch : 320 Loss : 3.308073526989907\n",
      "Epoch : 321 Loss : 3.306763201616873\n",
      "Epoch : 322 Loss : 3.305488312141375\n",
      "Epoch : 323 Loss : 3.3042288718034065\n",
      "Epoch : 324 Loss : 3.302946650320391\n",
      "Epoch : 325 Loss : 3.3017038323608223\n",
      "Epoch : 326 Loss : 3.3004403837048226\n",
      "Epoch : 327 Loss : 3.299175364667657\n",
      "Epoch : 328 Loss : 3.2979332901504135\n",
      "Epoch : 329 Loss : 3.2966751224757065\n",
      "Epoch : 330 Loss : 3.2954209018979603\n",
      "Epoch : 331 Loss : 3.2941604605519177\n",
      "Epoch : 332 Loss : 3.292940042274176\n",
      "Epoch : 333 Loss : 3.291685596028437\n",
      "Epoch : 334 Loss : 3.2904682911447947\n",
      "Epoch : 335 Loss : 3.2892273798611593\n",
      "Epoch : 336 Loss : 3.2880011013296913\n",
      "Epoch : 337 Loss : 3.286768295438235\n",
      "Epoch : 338 Loss : 3.2855565426198168\n",
      "Epoch : 339 Loss : 3.284335225090511\n",
      "Epoch : 340 Loss : 3.283128945114406\n",
      "Epoch : 341 Loss : 3.281918765219034\n",
      "Epoch : 342 Loss : 3.280714686676735\n",
      "Epoch : 343 Loss : 3.279520527381357\n",
      "Epoch : 344 Loss : 3.2783335840504444\n",
      "Epoch : 345 Loss : 3.2771534689902238\n",
      "Epoch : 346 Loss : 3.275944535914933\n",
      "Epoch : 347 Loss : 3.274775749896003\n",
      "Epoch : 348 Loss : 3.2735778624255865\n",
      "Epoch : 349 Loss : 3.272403602804668\n",
      "Epoch : 350 Loss : 3.27122899306452\n",
      "Epoch : 351 Loss : 3.2700724563752344\n",
      "Epoch : 352 Loss : 3.268916498043296\n",
      "Epoch : 353 Loss : 3.2677581370023474\n",
      "Epoch : 354 Loss : 3.266612866059582\n",
      "Epoch : 355 Loss : 3.2654531888946146\n",
      "Epoch : 356 Loss : 3.26432193562061\n",
      "Epoch : 357 Loss : 3.2631727828716315\n",
      "Epoch : 358 Loss : 3.2620277180749255\n",
      "Epoch : 359 Loss : 3.260856737561528\n",
      "Epoch : 360 Loss : 3.259725805252758\n",
      "Epoch : 361 Loss : 3.2586114083454008\n",
      "Epoch : 362 Loss : 3.2574892968163094\n",
      "Epoch : 363 Loss : 3.256366508597196\n",
      "Epoch : 364 Loss : 3.2552414124647955\n",
      "Epoch : 365 Loss : 3.254132521761248\n",
      "Epoch : 366 Loss : 3.25301450346116\n",
      "Epoch : 367 Loss : 3.2519633833793513\n",
      "Epoch : 368 Loss : 3.250888023763857\n",
      "Epoch : 369 Loss : 3.249809937241158\n",
      "Epoch : 370 Loss : 3.248764384163703\n",
      "Epoch : 371 Loss : 3.247734932507399\n",
      "Epoch : 372 Loss : 3.246729459687315\n",
      "Epoch : 373 Loss : 3.2456971830369494\n",
      "Epoch : 374 Loss : 3.244683695599481\n",
      "Epoch : 375 Loss : 3.243661200065795\n",
      "Epoch : 376 Loss : 3.242636619570001\n",
      "Epoch : 377 Loss : 3.2416374034541717\n",
      "Epoch : 378 Loss : 3.240632225421945\n",
      "Epoch : 379 Loss : 3.2396165956031697\n",
      "Epoch : 380 Loss : 3.2385788684467904\n",
      "Epoch : 381 Loss : 3.2375570220171395\n",
      "Epoch : 382 Loss : 3.236527064535031\n",
      "Epoch : 383 Loss : 3.2354982730611304\n",
      "Epoch : 384 Loss : 3.2345053935264763\n",
      "Epoch : 385 Loss : 3.233476759044664\n",
      "Epoch : 386 Loss : 3.2324702579542732\n",
      "Epoch : 387 Loss : 3.231457827863067\n",
      "Epoch : 388 Loss : 3.230440058213421\n",
      "Epoch : 389 Loss : 3.229444078095304\n",
      "Epoch : 390 Loss : 3.22845158683865\n",
      "Epoch : 391 Loss : 3.2274718747643676\n",
      "Epoch : 392 Loss : 3.2264547417528706\n",
      "Epoch : 393 Loss : 3.2254389459780435\n",
      "Epoch : 394 Loss : 3.2244429111894615\n",
      "Epoch : 395 Loss : 3.2234237231205594\n",
      "Epoch : 396 Loss : 3.2224343382168246\n",
      "Epoch : 397 Loss : 3.221435572154358\n",
      "Epoch : 398 Loss : 3.220428096563474\n",
      "Epoch : 399 Loss : 3.219437282060746\n",
      "Epoch : 400 Loss : 3.2184480730478766\n",
      "Epoch : 401 Loss : 3.2174264218483284\n",
      "Epoch : 402 Loss : 3.2164571019960246\n",
      "Epoch : 403 Loss : 3.2154628135650833\n",
      "Epoch : 404 Loss : 3.214460758300283\n",
      "Epoch : 405 Loss : 3.2134999144327043\n",
      "Epoch : 406 Loss : 3.212510044231839\n",
      "Epoch : 407 Loss : 3.2115194155941214\n",
      "Epoch : 408 Loss : 3.2105526596814324\n",
      "Epoch : 409 Loss : 3.20955782488306\n",
      "Epoch : 410 Loss : 3.2085586483072435\n",
      "Epoch : 411 Loss : 3.2075857983735476\n",
      "Epoch : 412 Loss : 3.2066062725014337\n",
      "Epoch : 413 Loss : 3.205632410567854\n",
      "Epoch : 414 Loss : 3.2046596741104674\n",
      "Epoch : 415 Loss : 3.2036809607493755\n",
      "Epoch : 416 Loss : 3.202722726880373\n",
      "Epoch : 417 Loss : 3.201739416481313\n",
      "Epoch : 418 Loss : 3.2007582473899263\n",
      "Epoch : 419 Loss : 3.199798351657798\n",
      "Epoch : 420 Loss : 3.1988155784065655\n",
      "Epoch : 421 Loss : 3.1978481705495523\n",
      "Epoch : 422 Loss : 3.1968842315265142\n",
      "Epoch : 423 Loss : 3.1958986948334185\n",
      "Epoch : 424 Loss : 3.194934199784057\n",
      "Epoch : 425 Loss : 3.1938886448104284\n",
      "Epoch : 426 Loss : 3.192859146528608\n",
      "Epoch : 427 Loss : 3.1918521638057293\n",
      "Epoch : 428 Loss : 3.190842005957894\n",
      "Epoch : 429 Loss : 3.189857419082438\n",
      "Epoch : 430 Loss : 3.1888736706372827\n",
      "Epoch : 431 Loss : 3.1878769060097527\n",
      "Epoch : 432 Loss : 3.186897295883728\n",
      "Epoch : 433 Loss : 3.185927764826884\n",
      "Epoch : 434 Loss : 3.1849480144129414\n",
      "Epoch : 435 Loss : 3.184011729162235\n",
      "Epoch : 436 Loss : 3.1830797825372983\n",
      "Epoch : 437 Loss : 3.1821364335297826\n",
      "Epoch : 438 Loss : 3.1812080472245245\n",
      "Epoch : 439 Loss : 3.180279950081883\n",
      "Epoch : 440 Loss : 3.1793437115021868\n",
      "Epoch : 441 Loss : 3.178433800829007\n",
      "Epoch : 442 Loss : 3.1775029625487123\n",
      "Epoch : 443 Loss : 3.1765854465997334\n",
      "Epoch : 444 Loss : 3.1756885006604723\n",
      "Epoch : 445 Loss : 3.1747581138989642\n",
      "Epoch : 446 Loss : 3.173852059809824\n",
      "Epoch : 447 Loss : 3.1729624450234737\n",
      "Epoch : 448 Loss : 3.172048761354963\n",
      "Epoch : 449 Loss : 3.1711673211963305\n",
      "Epoch : 450 Loss : 3.1702587111222233\n",
      "Epoch : 451 Loss : 3.1693426128161444\n",
      "Epoch : 452 Loss : 3.1684278233313865\n",
      "Epoch : 453 Loss : 3.167511788330659\n",
      "Epoch : 454 Loss : 3.166616609416416\n",
      "Epoch : 455 Loss : 3.1657237560902436\n",
      "Epoch : 456 Loss : 3.164853573460105\n",
      "Epoch : 457 Loss : 3.1639332584011135\n",
      "Epoch : 458 Loss : 3.1630714232930996\n",
      "Epoch : 459 Loss : 3.162222476465344\n",
      "Epoch : 460 Loss : 3.161397027839148\n",
      "Epoch : 461 Loss : 3.160572259009513\n",
      "Epoch : 462 Loss : 3.159720475068202\n",
      "Epoch : 463 Loss : 3.1589078382207743\n",
      "Epoch : 464 Loss : 3.1580846639511977\n",
      "Epoch : 465 Loss : 3.157248968854416\n",
      "Epoch : 466 Loss : 3.156430284694474\n",
      "Epoch : 467 Loss : 3.1556004701594786\n",
      "Epoch : 468 Loss : 3.1547846672509636\n",
      "Epoch : 469 Loss : 3.153961902101251\n",
      "Epoch : 470 Loss : 3.15314248564008\n",
      "Epoch : 471 Loss : 3.1523231489180223\n",
      "Epoch : 472 Loss : 3.1515025297226686\n",
      "Epoch : 473 Loss : 3.150708396995266\n",
      "Epoch : 474 Loss : 3.1499022123088842\n",
      "Epoch : 475 Loss : 3.1490954261783135\n",
      "Epoch : 476 Loss : 3.1482925802312125\n",
      "Epoch : 477 Loss : 3.1474955839043663\n",
      "Epoch : 478 Loss : 3.1467018817988297\n",
      "Epoch : 479 Loss : 3.1458906587861035\n",
      "Epoch : 480 Loss : 3.1451095615725086\n",
      "Epoch : 481 Loss : 3.144286075399294\n",
      "Epoch : 482 Loss : 3.143484462465135\n",
      "Epoch : 483 Loss : 3.142689634916792\n",
      "Epoch : 484 Loss : 3.141896491892288\n",
      "Epoch : 485 Loss : 3.1411035456732472\n",
      "Epoch : 486 Loss : 3.140271583840649\n",
      "Epoch : 487 Loss : 3.1394880321537264\n",
      "Epoch : 488 Loss : 3.1386926816619747\n",
      "Epoch : 489 Loss : 3.137907790082636\n",
      "Epoch : 490 Loss : 3.1371093628419535\n",
      "Epoch : 491 Loss : 3.136327662834959\n",
      "Epoch : 492 Loss : 3.1355640063659993\n",
      "Epoch : 493 Loss : 3.134776105666704\n",
      "Epoch : 494 Loss : 3.134000108069028\n",
      "Epoch : 495 Loss : 3.1332036053084855\n",
      "Epoch : 496 Loss : 3.1324396913210317\n",
      "Epoch : 497 Loss : 3.1316734511021034\n",
      "Epoch : 498 Loss : 3.1309054051079013\n",
      "Epoch : 499 Loss : 3.1301341866349395\n",
      "Epoch : 500 Loss : 3.1293808011099933\n",
      "Epoch : 501 Loss : 3.12861220504863\n",
      "Epoch : 502 Loss : 3.127868315704983\n",
      "Epoch : 503 Loss : 3.127096264296046\n",
      "Epoch : 504 Loss : 3.126329808068603\n",
      "Epoch : 505 Loss : 3.1255710416926363\n",
      "Epoch : 506 Loss : 3.124811393464035\n",
      "Epoch : 507 Loss : 3.1240562827982603\n",
      "Epoch : 508 Loss : 3.1232988438246068\n",
      "Epoch : 509 Loss : 3.1225352014023544\n",
      "Epoch : 510 Loss : 3.1217814654029095\n",
      "Epoch : 511 Loss : 3.1210077100898803\n",
      "Epoch : 512 Loss : 3.1202348067213874\n",
      "Epoch : 513 Loss : 3.1194593837705806\n",
      "Epoch : 514 Loss : 3.1186782040709815\n",
      "Epoch : 515 Loss : 3.1178966753102415\n",
      "Epoch : 516 Loss : 3.1170877182263577\n",
      "Epoch : 517 Loss : 3.11631306768931\n",
      "Epoch : 518 Loss : 3.1155035629443892\n",
      "Epoch : 519 Loss : 3.1147301414054187\n",
      "Epoch : 520 Loss : 3.113961994854038\n",
      "Epoch : 521 Loss : 3.113173039803997\n",
      "Epoch : 522 Loss : 3.112384305081168\n",
      "Epoch : 523 Loss : 3.111573358694964\n",
      "Epoch : 524 Loss : 3.110790258827771\n",
      "Epoch : 525 Loss : 3.1100003021614055\n",
      "Epoch : 526 Loss : 3.1092336138518504\n",
      "Epoch : 527 Loss : 3.1084472836217114\n",
      "Epoch : 528 Loss : 3.1076727260493326\n",
      "Epoch : 529 Loss : 3.1068979100977123\n",
      "Epoch : 530 Loss : 3.1061212132221465\n",
      "Epoch : 531 Loss : 3.1053543324438135\n",
      "Epoch : 532 Loss : 3.104595609572879\n",
      "Epoch : 533 Loss : 3.1038324018927606\n",
      "Epoch : 534 Loss : 3.1030753986927406\n",
      "Epoch : 535 Loss : 3.1023007115194194\n",
      "Epoch : 536 Loss : 3.101538091257987\n",
      "Epoch : 537 Loss : 3.100776904339167\n",
      "Epoch : 538 Loss : 3.100025570089086\n",
      "Epoch : 539 Loss : 3.099262841116588\n",
      "Epoch : 540 Loss : 3.0985224389036583\n",
      "Epoch : 541 Loss : 3.097756756621573\n",
      "Epoch : 542 Loss : 3.097005992541418\n",
      "Epoch : 543 Loss : 3.0962638171106427\n",
      "Epoch : 544 Loss : 3.095521784973735\n",
      "Epoch : 545 Loss : 3.0947888564519457\n",
      "Epoch : 546 Loss : 3.094054262671225\n",
      "Epoch : 547 Loss : 3.09333272883654\n",
      "Epoch : 548 Loss : 3.092604961944305\n",
      "Epoch : 549 Loss : 3.0918773465006892\n",
      "Epoch : 550 Loss : 3.0911841221888263\n",
      "Epoch : 551 Loss : 3.090452737594567\n",
      "Epoch : 552 Loss : 3.0897586559891135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 553 Loss : 3.089061061129036\n",
      "Epoch : 554 Loss : 3.0883579449605585\n",
      "Epoch : 555 Loss : 3.087694646131067\n",
      "Epoch : 556 Loss : 3.087010593365555\n",
      "Epoch : 557 Loss : 3.0863242204910915\n",
      "Epoch : 558 Loss : 3.0856190390257945\n",
      "Epoch : 559 Loss : 3.084936361252079\n",
      "Epoch : 560 Loss : 3.0842761393334777\n",
      "Epoch : 561 Loss : 3.0835639324547595\n",
      "Epoch : 562 Loss : 3.0828838988782445\n",
      "Epoch : 563 Loss : 3.082212217673474\n",
      "Epoch : 564 Loss : 3.0815279011705767\n",
      "Epoch : 565 Loss : 3.0808451658178315\n",
      "Epoch : 566 Loss : 3.080156829448767\n",
      "Epoch : 567 Loss : 3.079463194872099\n",
      "Epoch : 568 Loss : 3.078786692541805\n",
      "Epoch : 569 Loss : 3.078132122162493\n",
      "Epoch : 570 Loss : 3.0774829911549655\n",
      "Epoch : 571 Loss : 3.0767941576193936\n",
      "Epoch : 572 Loss : 3.0761316722742955\n",
      "Epoch : 573 Loss : 3.075482004721916\n",
      "Epoch : 574 Loss : 3.0748191224871033\n",
      "Epoch : 575 Loss : 3.074154496255461\n",
      "Epoch : 576 Loss : 3.0735103163017294\n",
      "Epoch : 577 Loss : 3.072855528232169\n",
      "Epoch : 578 Loss : 3.0722017949999922\n",
      "Epoch : 579 Loss : 3.071556955551492\n",
      "Epoch : 580 Loss : 3.070910473907402\n",
      "Epoch : 581 Loss : 3.070256010958333\n",
      "Epoch : 582 Loss : 3.0695959622762974\n",
      "Epoch : 583 Loss : 3.0689768974288114\n",
      "Epoch : 584 Loss : 3.0683180049800156\n",
      "Epoch : 585 Loss : 3.0676691041357063\n",
      "Epoch : 586 Loss : 3.0670362358586503\n",
      "Epoch : 587 Loss : 3.0663779259097215\n",
      "Epoch : 588 Loss : 3.065753878700572\n",
      "Epoch : 589 Loss : 3.0651222161873695\n",
      "Epoch : 590 Loss : 3.064472096788486\n",
      "Epoch : 591 Loss : 3.0638398372774347\n",
      "Epoch : 592 Loss : 3.0631950155863437\n",
      "Epoch : 593 Loss : 3.062568473189115\n",
      "Epoch : 594 Loss : 3.061922849032949\n",
      "Epoch : 595 Loss : 3.061290216650886\n",
      "Epoch : 596 Loss : 3.060666208321924\n",
      "Epoch : 597 Loss : 3.060024743639692\n",
      "Epoch : 598 Loss : 3.0593995367576383\n",
      "Epoch : 599 Loss : 3.0587775949344906\n",
      "Epoch : 600 Loss : 3.058143897963864\n",
      "Epoch : 601 Loss : 3.057508116945295\n",
      "Epoch : 602 Loss : 3.0568984299890025\n",
      "Epoch : 603 Loss : 3.0562585410415415\n",
      "Epoch : 604 Loss : 3.0556397171414686\n",
      "Epoch : 605 Loss : 3.055015931892795\n",
      "Epoch : 606 Loss : 3.0543941355754285\n",
      "Epoch : 607 Loss : 3.0537736129025066\n",
      "Epoch : 608 Loss : 3.05314120353481\n",
      "Epoch : 609 Loss : 3.0525228769249466\n",
      "Epoch : 610 Loss : 3.0519125716023354\n",
      "Epoch : 611 Loss : 3.05127086183789\n",
      "Epoch : 612 Loss : 3.050672561767439\n",
      "Epoch : 613 Loss : 3.0500544790487645\n",
      "Epoch : 614 Loss : 3.049420534418787\n",
      "Epoch : 615 Loss : 3.048805506101513\n",
      "Epoch : 616 Loss : 3.048215359828928\n",
      "Epoch : 617 Loss : 3.0475798528379707\n",
      "Epoch : 618 Loss : 3.0469777641375635\n",
      "Epoch : 619 Loss : 3.0463617036788144\n",
      "Epoch : 620 Loss : 3.0457499752232375\n",
      "Epoch : 621 Loss : 3.045176375055962\n",
      "Epoch : 622 Loss : 3.044566299565644\n",
      "Epoch : 623 Loss : 3.0439554917194966\n",
      "Epoch : 624 Loss : 3.0433667800214104\n",
      "Epoch : 625 Loss : 3.042769104768276\n",
      "Epoch : 626 Loss : 3.042161303161884\n",
      "Epoch : 627 Loss : 3.041571612225008\n",
      "Epoch : 628 Loss : 3.040971284952617\n",
      "Epoch : 629 Loss : 3.0403965298632754\n",
      "Epoch : 630 Loss : 3.039810045064585\n",
      "Epoch : 631 Loss : 3.039226861547084\n",
      "Epoch : 632 Loss : 3.038646275441063\n",
      "Epoch : 633 Loss : 3.038067686614805\n",
      "Epoch : 634 Loss : 3.0374741928952336\n",
      "Epoch : 635 Loss : 3.0369003071213467\n",
      "Epoch : 636 Loss : 3.0363107460403014\n",
      "Epoch : 637 Loss : 3.0357303530237463\n",
      "Epoch : 638 Loss : 3.035149585469518\n",
      "Epoch : 639 Loss : 3.034563563297523\n",
      "Epoch : 640 Loss : 3.033996149188561\n",
      "Epoch : 641 Loss : 3.0333928212757173\n",
      "Epoch : 642 Loss : 3.03278238803482\n",
      "Epoch : 643 Loss : 3.032192955732432\n",
      "Epoch : 644 Loss : 3.0315851686928945\n",
      "Epoch : 645 Loss : 3.0309805834444234\n",
      "Epoch : 646 Loss : 3.030382183634618\n",
      "Epoch : 647 Loss : 3.0297916519051413\n",
      "Epoch : 648 Loss : 3.0291824216395753\n",
      "Epoch : 649 Loss : 3.028583826244531\n",
      "Epoch : 650 Loss : 3.0279827990868515\n",
      "Epoch : 651 Loss : 3.0273934997613803\n",
      "Epoch : 652 Loss : 3.026797950773272\n",
      "Epoch : 653 Loss : 3.0261985500489517\n",
      "Epoch : 654 Loss : 3.0256050534420744\n",
      "Epoch : 655 Loss : 3.0250105804061613\n",
      "Epoch : 656 Loss : 3.024415980154971\n",
      "Epoch : 657 Loss : 3.02381943277975\n",
      "Epoch : 658 Loss : 3.023215250699548\n",
      "Epoch : 659 Loss : 3.0226073359828995\n",
      "Epoch : 660 Loss : 3.022024238110653\n",
      "Epoch : 661 Loss : 3.021408885629119\n",
      "Epoch : 662 Loss : 3.0208293983767196\n",
      "Epoch : 663 Loss : 3.0202205696392044\n",
      "Epoch : 664 Loss : 3.0196222312339374\n",
      "Epoch : 665 Loss : 3.0190320739532504\n",
      "Epoch : 666 Loss : 3.018430994122772\n",
      "Epoch : 667 Loss : 3.017839993663852\n",
      "Epoch : 668 Loss : 3.0172489201382158\n",
      "Epoch : 669 Loss : 3.016645354184817\n",
      "Epoch : 670 Loss : 3.016037113383052\n",
      "Epoch : 671 Loss : 3.0154787414169175\n",
      "Epoch : 672 Loss : 3.0148719609492254\n",
      "Epoch : 673 Loss : 3.014285199490857\n",
      "Epoch : 674 Loss : 3.013700367992288\n",
      "Epoch : 675 Loss : 3.0131300018675837\n",
      "Epoch : 676 Loss : 3.012563608762912\n",
      "Epoch : 677 Loss : 3.01197902643888\n",
      "Epoch : 678 Loss : 3.011405163984861\n",
      "Epoch : 679 Loss : 3.0108347730083014\n",
      "Epoch : 680 Loss : 3.010264231040868\n",
      "Epoch : 681 Loss : 3.009683478623459\n",
      "Epoch : 682 Loss : 3.0091054322181123\n",
      "Epoch : 683 Loss : 3.008511202857318\n",
      "Epoch : 684 Loss : 3.0079332136029056\n",
      "Epoch : 685 Loss : 3.00733366160489\n",
      "Epoch : 686 Loss : 3.0067482544252035\n",
      "Epoch : 687 Loss : 3.0061775022191237\n",
      "Epoch : 688 Loss : 3.0055905091477086\n",
      "Epoch : 689 Loss : 3.0050144964653365\n",
      "Epoch : 690 Loss : 3.0044251573366636\n",
      "Epoch : 691 Loss : 3.0038453585991296\n",
      "Epoch : 692 Loss : 3.003275400535596\n",
      "Epoch : 693 Loss : 3.002702582284754\n",
      "Epoch : 694 Loss : 3.0021419250883414\n",
      "Epoch : 695 Loss : 3.0015549208845598\n",
      "Epoch : 696 Loss : 3.001013544035764\n",
      "Epoch : 697 Loss : 3.0004553822295774\n",
      "Epoch : 698 Loss : 2.999900324751308\n",
      "Epoch : 699 Loss : 2.999350162855886\n",
      "Epoch : 700 Loss : 2.998781369116696\n",
      "Epoch : 701 Loss : 2.9982369851379707\n",
      "Epoch : 702 Loss : 2.997682481252965\n",
      "Epoch : 703 Loss : 2.9971365995383903\n",
      "Epoch : 704 Loss : 2.996576426722542\n",
      "Epoch : 705 Loss : 2.9960346777931854\n",
      "Epoch : 706 Loss : 2.9954773063633735\n",
      "Epoch : 707 Loss : 2.994917897774191\n",
      "Epoch : 708 Loss : 2.9943809086920754\n",
      "Epoch : 709 Loss : 2.99382531370718\n",
      "Epoch : 710 Loss : 2.993287705331785\n",
      "Epoch : 711 Loss : 2.9927606504986914\n",
      "Epoch : 712 Loss : 2.9922049256449994\n",
      "Epoch : 713 Loss : 2.991672496916646\n",
      "Epoch : 714 Loss : 2.991124698210071\n",
      "Epoch : 715 Loss : 2.990602216396086\n",
      "Epoch : 716 Loss : 2.990044554803991\n",
      "Epoch : 717 Loss : 2.9895095726524716\n",
      "Epoch : 718 Loss : 2.9889744650138774\n",
      "Epoch : 719 Loss : 2.9884398175062965\n",
      "Epoch : 720 Loss : 2.987907366823533\n",
      "Epoch : 721 Loss : 2.9873488423217127\n",
      "Epoch : 722 Loss : 2.9868285038951026\n",
      "Epoch : 723 Loss : 2.9862782879240557\n",
      "Epoch : 724 Loss : 2.9857579668254783\n",
      "Epoch : 725 Loss : 2.985224097911061\n",
      "Epoch : 726 Loss : 2.984676629558555\n",
      "Epoch : 727 Loss : 2.984180070901769\n",
      "Epoch : 728 Loss : 2.98362272051041\n",
      "Epoch : 729 Loss : 2.9831060622853705\n",
      "Epoch : 730 Loss : 2.9825881539511747\n",
      "Epoch : 731 Loss : 2.9820471999403546\n",
      "Epoch : 732 Loss : 2.9815342551777464\n",
      "Epoch : 733 Loss : 2.9810091859832584\n",
      "Epoch : 734 Loss : 2.980496238560083\n",
      "Epoch : 735 Loss : 2.9799933048875644\n",
      "Epoch : 736 Loss : 2.979454463804866\n",
      "Epoch : 737 Loss : 2.9789366952100527\n",
      "Epoch : 738 Loss : 2.978420799658239\n",
      "Epoch : 739 Loss : 2.977941942004781\n",
      "Epoch : 740 Loss : 2.9774306141157827\n",
      "Epoch : 741 Loss : 2.976917903799436\n",
      "Epoch : 742 Loss : 2.9763857639629996\n",
      "Epoch : 743 Loss : 2.975895858591848\n",
      "Epoch : 744 Loss : 2.9753645415901055\n",
      "Epoch : 745 Loss : 2.9748544471939753\n",
      "Epoch : 746 Loss : 2.97433070707182\n",
      "Epoch : 747 Loss : 2.973803041771035\n",
      "Epoch : 748 Loss : 2.973305845265786\n",
      "Epoch : 749 Loss : 2.972772962345108\n",
      "Epoch : 750 Loss : 2.972262036320072\n",
      "Epoch : 751 Loss : 2.971749957524214\n",
      "Epoch : 752 Loss : 2.971252598481565\n",
      "Epoch : 753 Loss : 2.9707405923090167\n",
      "Epoch : 754 Loss : 2.9702158020985334\n",
      "Epoch : 755 Loss : 2.9697373671715717\n",
      "Epoch : 756 Loss : 2.9692048450225794\n",
      "Epoch : 757 Loss : 2.9687167159167585\n",
      "Epoch : 758 Loss : 2.9682308039523035\n",
      "Epoch : 759 Loss : 2.9677081082544756\n",
      "Epoch : 760 Loss : 2.9672355121144096\n",
      "Epoch : 761 Loss : 2.9667143219767578\n",
      "Epoch : 762 Loss : 2.966219775868642\n",
      "Epoch : 763 Loss : 2.965702278952464\n",
      "Epoch : 764 Loss : 2.965205919899278\n",
      "Epoch : 765 Loss : 2.9646855027692958\n",
      "Epoch : 766 Loss : 2.9642241059672436\n",
      "Epoch : 767 Loss : 2.9636966517858765\n",
      "Epoch : 768 Loss : 2.963201069629\n",
      "Epoch : 769 Loss : 2.9626736782689846\n",
      "Epoch : 770 Loss : 2.9621746254237418\n",
      "Epoch : 771 Loss : 2.9616807696344534\n",
      "Epoch : 772 Loss : 2.961174285254677\n",
      "Epoch : 773 Loss : 2.960657178146106\n",
      "Epoch : 774 Loss : 2.960152349490328\n",
      "Epoch : 775 Loss : 2.95967163178869\n",
      "Epoch : 776 Loss : 2.959192924219178\n",
      "Epoch : 777 Loss : 2.9586828832230045\n",
      "Epoch : 778 Loss : 2.9581936781167935\n",
      "Epoch : 779 Loss : 2.9577000679759666\n",
      "Epoch : 780 Loss : 2.957185934649609\n",
      "Epoch : 781 Loss : 2.9567061898054643\n",
      "Epoch : 782 Loss : 2.9561952519769483\n",
      "Epoch : 783 Loss : 2.955728505463361\n",
      "Epoch : 784 Loss : 2.9552222535159847\n",
      "Epoch : 785 Loss : 2.9547482158082548\n",
      "Epoch : 786 Loss : 2.954238412475425\n",
      "Epoch : 787 Loss : 2.9537576117181277\n",
      "Epoch : 788 Loss : 2.9532638409501044\n",
      "Epoch : 789 Loss : 2.952786445479859\n",
      "Epoch : 790 Loss : 2.9522781163316845\n",
      "Epoch : 791 Loss : 2.9517964442992946\n",
      "Epoch : 792 Loss : 2.9513204728963354\n",
      "Epoch : 793 Loss : 2.950836860365762\n",
      "Epoch : 794 Loss : 2.9503641780695338\n",
      "Epoch : 795 Loss : 2.949887367609987\n",
      "Epoch : 796 Loss : 2.949399277303849\n",
      "Epoch : 797 Loss : 2.9489111991453347\n",
      "Epoch : 798 Loss : 2.948441886656269\n",
      "Epoch : 799 Loss : 2.9479546119327105\n",
      "Epoch : 800 Loss : 2.947497262761323\n",
      "Epoch : 801 Loss : 2.947005484810407\n",
      "Epoch : 802 Loss : 2.946535505720035\n",
      "Epoch : 803 Loss : 2.946073913370801\n",
      "Epoch : 804 Loss : 2.945579762729796\n",
      "Epoch : 805 Loss : 2.94511548942179\n",
      "Epoch : 806 Loss : 2.9446409754376397\n",
      "Epoch : 807 Loss : 2.944165861505961\n",
      "Epoch : 808 Loss : 2.943690544086083\n",
      "Epoch : 809 Loss : 2.9432343205174973\n",
      "Epoch : 810 Loss : 2.942750861780149\n",
      "Epoch : 811 Loss : 2.9423099274009816\n",
      "Epoch : 812 Loss : 2.941822980734293\n",
      "Epoch : 813 Loss : 2.9413438127291367\n",
      "Epoch : 814 Loss : 2.9409075165606646\n",
      "Epoch : 815 Loss : 2.940420893902854\n",
      "Epoch : 816 Loss : 2.939980985525882\n",
      "Epoch : 817 Loss : 2.939498368713862\n",
      "Epoch : 818 Loss : 2.939034758026475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 819 Loss : 2.9385878377679684\n",
      "Epoch : 820 Loss : 2.9380970401154283\n",
      "Epoch : 821 Loss : 2.9376093953327933\n",
      "Epoch : 822 Loss : 2.937175098172024\n",
      "Epoch : 823 Loss : 2.936659084052811\n",
      "Epoch : 824 Loss : 2.9362302607129664\n",
      "Epoch : 825 Loss : 2.9357540529380826\n",
      "Epoch : 826 Loss : 2.9352534431698625\n",
      "Epoch : 827 Loss : 2.9347602767974554\n",
      "Epoch : 828 Loss : 2.934289998170412\n",
      "Epoch : 829 Loss : 2.9338256881733114\n",
      "Epoch : 830 Loss : 2.9333367417792355\n",
      "Epoch : 831 Loss : 2.932844263858304\n",
      "Epoch : 832 Loss : 2.9323777715850405\n",
      "Epoch : 833 Loss : 2.9319176143583996\n",
      "Epoch : 834 Loss : 2.931421460427583\n",
      "Epoch : 835 Loss : 2.930974177283116\n",
      "Epoch : 836 Loss : 2.930528801660521\n",
      "Epoch : 837 Loss : 2.9300623474275898\n",
      "Epoch : 838 Loss : 2.9295919495795384\n",
      "Epoch : 839 Loss : 2.9291416908731236\n",
      "Epoch : 840 Loss : 2.928692680176314\n",
      "Epoch : 841 Loss : 2.92824333379295\n",
      "Epoch : 842 Loss : 2.927780330085541\n",
      "Epoch : 843 Loss : 2.927330942262446\n",
      "Epoch : 844 Loss : 2.926888246699745\n",
      "Epoch : 845 Loss : 2.926420761859389\n",
      "Epoch : 846 Loss : 2.9259704575303633\n",
      "Epoch : 847 Loss : 2.9255310830252403\n",
      "Epoch : 848 Loss : 2.9250754435911213\n",
      "Epoch : 849 Loss : 2.92460495426871\n",
      "Epoch : 850 Loss : 2.9241674642178856\n",
      "Epoch : 851 Loss : 2.923704575429033\n",
      "Epoch : 852 Loss : 2.923276346060807\n",
      "Epoch : 853 Loss : 2.922811456447568\n",
      "Epoch : 854 Loss : 2.922371577317305\n",
      "Epoch : 855 Loss : 2.921904118017994\n",
      "Epoch : 856 Loss : 2.921471092430935\n",
      "Epoch : 857 Loss : 2.9210033244641553\n",
      "Epoch : 858 Loss : 2.9205661947655126\n",
      "Epoch : 859 Loss : 2.9201122147657026\n",
      "Epoch : 860 Loss : 2.9196494087333793\n",
      "Epoch : 861 Loss : 2.9192016471287996\n",
      "Epoch : 862 Loss : 2.9187364980140593\n",
      "Epoch : 863 Loss : 2.9182724623152745\n",
      "Epoch : 864 Loss : 2.9178279252465393\n",
      "Epoch : 865 Loss : 2.917361351040449\n",
      "Epoch : 866 Loss : 2.9169127511870183\n",
      "Epoch : 867 Loss : 2.9164386078569655\n",
      "Epoch : 868 Loss : 2.915993948237563\n",
      "Epoch : 869 Loss : 2.9155243452866197\n",
      "Epoch : 870 Loss : 2.9150890565686702\n",
      "Epoch : 871 Loss : 2.914615486372269\n",
      "Epoch : 872 Loss : 2.9141912863226738\n",
      "Epoch : 873 Loss : 2.913706225382561\n",
      "Epoch : 874 Loss : 2.9132582519182613\n",
      "Epoch : 875 Loss : 2.9128164929006624\n",
      "Epoch : 876 Loss : 2.9123645937125135\n",
      "Epoch : 877 Loss : 2.911896485998182\n",
      "Epoch : 878 Loss : 2.9114429132872655\n",
      "Epoch : 879 Loss : 2.910991476571171\n",
      "Epoch : 880 Loss : 2.9105297712659826\n",
      "Epoch : 881 Loss : 2.910079710912158\n",
      "Epoch : 882 Loss : 2.909624650226664\n",
      "Epoch : 883 Loss : 2.909189677447668\n",
      "Epoch : 884 Loss : 2.908736187882397\n",
      "Epoch : 885 Loss : 2.908273977389853\n",
      "Epoch : 886 Loss : 2.9078330131676884\n",
      "Epoch : 887 Loss : 2.9073870962424135\n",
      "Epoch : 888 Loss : 2.9069318090255223\n",
      "Epoch : 889 Loss : 2.906474573262091\n",
      "Epoch : 890 Loss : 2.906027320187341\n",
      "Epoch : 891 Loss : 2.9055599878320777\n",
      "Epoch : 892 Loss : 2.9050883421782023\n",
      "Epoch : 893 Loss : 2.9046589996349637\n",
      "Epoch : 894 Loss : 2.904182353330815\n",
      "Epoch : 895 Loss : 2.9037372854708545\n",
      "Epoch : 896 Loss : 2.9032797274658013\n",
      "Epoch : 897 Loss : 2.9028516148566785\n",
      "Epoch : 898 Loss : 2.9023675240930706\n",
      "Epoch : 899 Loss : 2.9019322532448335\n",
      "Epoch : 900 Loss : 2.9014764655801875\n",
      "Epoch : 901 Loss : 2.901035598507361\n",
      "Epoch : 902 Loss : 2.9005633533731126\n",
      "Epoch : 903 Loss : 2.900122281608546\n",
      "Epoch : 904 Loss : 2.899673935237224\n",
      "Epoch : 905 Loss : 2.8992058868304986\n",
      "Epoch : 906 Loss : 2.8987804851600267\n",
      "Epoch : 907 Loss : 2.8982944228724947\n",
      "Epoch : 908 Loss : 2.8978650340893584\n",
      "Epoch : 909 Loss : 2.8974123848900746\n",
      "Epoch : 910 Loss : 2.8969532410677075\n",
      "Epoch : 911 Loss : 2.8965073375935324\n",
      "Epoch : 912 Loss : 2.896075878538333\n",
      "Epoch : 913 Loss : 2.895618546600573\n",
      "Epoch : 914 Loss : 2.8951606569265667\n",
      "Epoch : 915 Loss : 2.8947386140608047\n",
      "Epoch : 916 Loss : 2.894267955295662\n",
      "Epoch : 917 Loss : 2.893854724610747\n",
      "Epoch : 918 Loss : 2.893393909555113\n",
      "Epoch : 919 Loss : 2.892946410795606\n",
      "Epoch : 920 Loss : 2.8925227187610187\n",
      "Epoch : 921 Loss : 2.892040634907861\n",
      "Epoch : 922 Loss : 2.8915905773234574\n",
      "Epoch : 923 Loss : 2.8911165973526987\n",
      "Epoch : 924 Loss : 2.8906767432632154\n",
      "Epoch : 925 Loss : 2.890218203677644\n",
      "Epoch : 926 Loss : 2.889748696658808\n",
      "Epoch : 927 Loss : 2.8892831100074168\n",
      "Epoch : 928 Loss : 2.8888282742384597\n",
      "Epoch : 929 Loss : 2.8883680999482038\n",
      "Epoch : 930 Loss : 2.8879105762310178\n",
      "Epoch : 931 Loss : 2.8874886399008766\n",
      "Epoch : 932 Loss : 2.887010614169466\n",
      "Epoch : 933 Loss : 2.8865800206586423\n",
      "Epoch : 934 Loss : 2.8861151753319203\n",
      "Epoch : 935 Loss : 2.8856694826090856\n",
      "Epoch : 936 Loss : 2.8852201453479416\n",
      "Epoch : 937 Loss : 2.884777435208307\n",
      "Epoch : 938 Loss : 2.884343375396844\n",
      "Epoch : 939 Loss : 2.8838824319014305\n",
      "Epoch : 940 Loss : 2.8834831841348585\n",
      "Epoch : 941 Loss : 2.8830095586793014\n",
      "Epoch : 942 Loss : 2.8825941458711726\n",
      "Epoch : 943 Loss : 2.8821571640484605\n",
      "Epoch : 944 Loss : 2.8816940804941638\n",
      "Epoch : 945 Loss : 2.881279650932591\n",
      "Epoch : 946 Loss : 2.880828038219454\n",
      "Epoch : 947 Loss : 2.8803769767994556\n",
      "Epoch : 948 Loss : 2.879956340155817\n",
      "Epoch : 949 Loss : 2.8795227612964966\n",
      "Epoch : 950 Loss : 2.879080550279528\n",
      "Epoch : 951 Loss : 2.8786725710673413\n",
      "Epoch : 952 Loss : 2.8782167413639406\n",
      "Epoch : 953 Loss : 2.877812696247622\n",
      "Epoch : 954 Loss : 2.8773971962690834\n",
      "Epoch : 955 Loss : 2.8769671838911224\n",
      "Epoch : 956 Loss : 2.876542258820616\n",
      "Epoch : 957 Loss : 2.8761351847425107\n",
      "Epoch : 958 Loss : 2.875684797609747\n",
      "Epoch : 959 Loss : 2.8752727287274897\n",
      "Epoch : 960 Loss : 2.8748510719694154\n",
      "Epoch : 961 Loss : 2.8744115939492767\n",
      "Epoch : 962 Loss : 2.8739940947087352\n",
      "Epoch : 963 Loss : 2.8735836249766846\n",
      "Epoch : 964 Loss : 2.8731527674425785\n",
      "Epoch : 965 Loss : 2.872752539184657\n",
      "Epoch : 966 Loss : 2.8723691529205655\n",
      "Epoch : 967 Loss : 2.871962748210874\n",
      "Epoch : 968 Loss : 2.871547663316205\n",
      "Epoch : 969 Loss : 2.8711460050556292\n",
      "Epoch : 970 Loss : 2.870761005664592\n",
      "Epoch : 971 Loss : 2.8703507213920516\n",
      "Epoch : 972 Loss : 2.869960461823916\n",
      "Epoch : 973 Loss : 2.8695505375119335\n",
      "Epoch : 974 Loss : 2.869168659578464\n",
      "Epoch : 975 Loss : 2.8687316579436857\n",
      "Epoch : 976 Loss : 2.868357933418474\n",
      "Epoch : 977 Loss : 2.8679460150020217\n",
      "Epoch : 978 Loss : 2.867574684397784\n",
      "Epoch : 979 Loss : 2.8671579314377698\n",
      "Epoch : 980 Loss : 2.8667636990796144\n",
      "Epoch : 981 Loss : 2.8663547911676313\n",
      "Epoch : 982 Loss : 2.86597591738065\n",
      "Epoch : 983 Loss : 2.8655618314305547\n",
      "Epoch : 984 Loss : 2.8651752124615077\n",
      "Epoch : 985 Loss : 2.8647866746056194\n",
      "Epoch : 986 Loss : 2.864389231081213\n",
      "Epoch : 987 Loss : 2.8639731723531985\n",
      "Epoch : 988 Loss : 2.863617218246281\n",
      "Epoch : 989 Loss : 2.863208155367653\n",
      "Epoch : 990 Loss : 2.862821773897707\n",
      "Epoch : 991 Loss : 2.8624358234019103\n",
      "Epoch : 992 Loss : 2.8620620601570934\n",
      "Epoch : 993 Loss : 2.8616492845215666\n",
      "Epoch : 994 Loss : 2.8612819760228647\n",
      "Epoch : 995 Loss : 2.860892910311879\n",
      "Epoch : 996 Loss : 2.8605084788421253\n",
      "Epoch : 997 Loss : 2.8601178062043022\n",
      "Epoch : 998 Loss : 2.859729410027865\n",
      "Epoch : 999 Loss : 2.859348021597936\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "# (batch, 9)\n",
    "x = normed_train_data.to_numpy()\n",
    "# (batch, 1)\n",
    "t = train_labels.to_numpy().reshape(-1, 1)\n",
    "\n",
    "from tqdm import tqdm\n",
    "for i in range(1000):\n",
    "    \n",
    "    # Forward Pass\n",
    "    # (batch, 1)\n",
    "    y = model.forward(x)\n",
    "    \n",
    "    # Get Loss\n",
    "    loss = MeanSquareError.get_loss(y, t)\n",
    "    print('Epoch :', i, 'Loss :', loss)\n",
    "    \n",
    "    # Backward Pass\n",
    "    grad = MeanSquareError.get_gradient(y, t)\n",
    "    model.backward(grad, learning_rate=1e-5)\n",
    "    \n",
    "    # Append loss\n",
    "    losses.append(loss)\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_test = normed_test_data.to_numpy()\n",
    "t_test = test_labels.to_numpy().reshape(-1, 1)\n",
    "y_test = model.forward(x_test)\n",
    "\n",
    "plt.scatter(t_test[:, 0], y_test[:,0 ])\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "_ = plt.plot([-100, 100], [-100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
